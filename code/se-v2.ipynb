{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_list = list()\n",
    "neg_list = list()\n",
    "rev_list = list()\n",
    "inc_list = list()\n",
    "dec_list = list()\n",
    "sent_words_dict = dict()\n",
    "\n",
    "\n",
    "fneg = open('../dict/negative-words.txt', 'r')\n",
    "fpos = open('../dict/positive-words.txt', 'r')\n",
    "frev = open('../dict/reverse-words.txt', 'r')\n",
    "fdec = open('../dict/decremental-words.txt', 'r')\n",
    "finc = open('../dict/incremental-words.txt', 'r')\n",
    "\n",
    "for line in fpos:\n",
    "    if not line.split()[0] in sent_words_dict:\n",
    "        sent_words_dict[line.split()[0]] = 0\n",
    "        pos_list.append(line.split()[0])\n",
    "for line in fneg:\n",
    "    if not line.split()[0] in sent_words_dict:\n",
    "        sent_words_dict[line.split()[0]] = 1\n",
    "        neg_list.append(line.split()[0])\n",
    "for line in frev:\n",
    "    if not line.split()[0] in sent_words_dict:\n",
    "        sent_words_dict[line.split()[0]] = 2\n",
    "        rev_list.append(line.split()[0])\n",
    "for line in finc:\n",
    "    if not line.split()[0] in sent_words_dict:\n",
    "        sent_words_dict[line.split()[0]] = 3\n",
    "        inc_list.append(line.split()[0])\n",
    "for line in fdec:\n",
    "    if not line.split()[0] in sent_words_dict:\n",
    "        sent_words_dict[line.split()[0]] = 4\n",
    "        dec_list.append(line.split()[0])\n",
    "            \n",
    "fneg.close()\n",
    "fpos.close()\n",
    "frev.close()\n",
    "fdec.close()\n",
    "finc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "X_train_labels = list()\n",
    "sent_dict = dict()\n",
    "sentences = list()\n",
    "\n",
    "sent_dict['positive'] = 0\n",
    "sent_dict['negative'] = 1\n",
    "sent_dict['neutral'] = 2\n",
    "sent_dict['objective'] = 2\n",
    "sent_dict['objective-OR-neutral'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 6087 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading CSV file...\"\n",
    "with open('../data/semeval/2013/b.dist.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    for x in reader:\n",
    "        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        sentences.append(re.sub(r\"http\\S+\", \"\", x[3]).decode('utf-8').lower())\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        X_train_labels.append(sent_dict[x[2]])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "\n",
    "f.close()\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16824 unique words tokens.\n",
      "Using vocabulary size 15399.\n",
      "The least frequent word in our vocabulary is 'alexisnews' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    " \n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab if re.search(r'^[a-zA-Z]', x[0]) and len(x[0]) < 20 and len(x[0]) > 1]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "word_to_index_reverse = {v: k for k, v in word_to_index.iteritems()}\n",
    "\n",
    "vocabulary_size = len(word_to_index)\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START iranian general says israel's iron dome can't deal with their missiles (keep talking like that and we may end up finding out) SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'iranian', u'general', u'says', u'israel', u'iron', u'dome', u'ca', u\"n't\", u'deal', u'with', u'their', u'missiles', u'keep', u'talking', u'like', u'that', u'and', u'we', u'may', u'end', u'up', u'finding', u'out', u'SENTENCE_END', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN']'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:29: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START iranian general says israel iron dome ca n't deal with their missiles keep talking like that and we may end up finding out SENTENCE_END UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      "[0, 7457, 2283, 214, 804, 4654, 4323, 93, 23, 659, 11, 131, 7047, 343, 924, 53, 16, 6, 28, 24, 220, 40, 4051, 27, 1, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398]\n",
      "\n",
      "y:\n",
      "iranian general says israel iron dome ca n't deal with their missiles keep talking like that and we may end up finding out SENTENCE_END UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      "[7457, 2283, 214, 804, 4654, 4323, 93, 23, 659, 11, 131, 7047, 343, 924, 53, 16, 6, 28, 24, 220, 40, 4051, 27, 1, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398]\n"
     ]
    }
   ],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w for w in sent if w in word_to_index]\n",
    "    \n",
    "    if (len(tokenized_sentences[i]) < 40):\n",
    "        while (len(tokenized_sentences[i]) < 40):\n",
    "            tokenized_sentences[i].append(unknown_token)\n",
    "    else:\n",
    "        while (len(tokenized_sentences[i]) > 40):\n",
    "            tokenized_sentences[i].pop()\n",
    "    \n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]\n",
    "\n",
    "X_train = list()\n",
    "X_train_sent_for_word = list()\n",
    "X_train_mask = list()\n",
    "\n",
    "# Create the training data\n",
    "for sent in tokenized_sentences:\n",
    "    temp_index = list()\n",
    "    temp_sent = list()\n",
    "    temp_mask = list()\n",
    "    for w in sent[:-1]:\n",
    "        temp_index.append(word_to_index[w])\n",
    "        if (w in pos_list):\n",
    "            temp_sent.append(0)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in neg_list):\n",
    "            temp_sent.append(1)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in rev_list):\n",
    "            temp_sent.append(2)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in inc_list):\n",
    "            temp_sent.append(3)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in dec_list):\n",
    "            temp_sent.append(4)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w == unknown_token):\n",
    "            temp_sent.append(5)\n",
    "            temp_mask.append(0.)\n",
    "        else:\n",
    "            temp_sent.append(5)\n",
    "            temp_mask.append(0.)\n",
    "            \n",
    "    X_train.append(temp_index)\n",
    "    X_train_sent_for_word.append(temp_sent)\n",
    "    X_train_mask.append(temp_mask)\n",
    "    \n",
    "#X_train = np.asarray(X_train)\n",
    "#X_train_sent_for_word = np.asarray(X_train_sent_for_word)\n",
    "#X_train_mask = np.asarray(X_train_mask)\n",
    "\n",
    "y_train = [[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences]\n",
    "\n",
    "# Print an training data example\n",
    "x_example, y_example = X_train[0], y_train[0]\n",
    "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2345 sentences.\n",
      "\n",
      "Example sentence: 'SENTENCE_START on radio786 100.4fm 7:10 fri oct 19 labour analyst shawn hattingh: cosatu's role in the context of unrest in the mining  SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'on', u'fri', u'oct', u'labour', u'shawn', u'role', u'in', u'the', u'of', u'in', u'the', u'mining', u'SENTENCE_END', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN']'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:45: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START on fri oct labour shawn role in the of in the mining SENTENCE_END UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      "[0, 5, 1024, 174, 9783, 4309, 2961, 4, 2, 7, 4, 2, 4818, 1, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398]\n",
      "\n",
      "y:\n",
      "on fri oct labour shawn role in the of in the mining SENTENCE_END UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      "[5, 1024, 174, 9783, 4309, 2961, 4, 2, 7, 4, 2, 4818, 1, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398, 15398]\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "X_test_labels = list()\n",
    "with open('../data/semeval/2013/b.test.dist.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    for x in reader:\n",
    "        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        sentences.append(re.sub(r\"http\\S+\", \"\", x[3]).decode('utf-8').lower())\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        X_test_labels.append(sent_dict[x[2]])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    \n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "f.close()\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w for w in sent if w in word_to_index]\n",
    "    if (len(tokenized_sentences[i]) < 40):\n",
    "        while (len(tokenized_sentences[i]) < 40):\n",
    "            tokenized_sentences[i].append(unknown_token)\n",
    "    else:\n",
    "        while (len(tokenized_sentences[i]) > 40):\n",
    "            tokenized_sentences[i].pop()\n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]\n",
    "\n",
    "X_test = list()\n",
    "X_test_sent_for_word = list()\n",
    "X_test_mask = list()\n",
    "\n",
    "# Create the training data\n",
    "for sent in tokenized_sentences:\n",
    "    temp_index = list()\n",
    "    temp_sent = list()\n",
    "    temp_mask = list()\n",
    "    for w in sent[:-1]:\n",
    "        temp_index.append(word_to_index[w])\n",
    "        if (w in pos_list):\n",
    "            temp_sent.append(0)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in neg_list):\n",
    "            temp_sent.append(1)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in rev_list):\n",
    "            temp_sent.append(2)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in inc_list):\n",
    "            temp_sent.append(3)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w in dec_list):\n",
    "            temp_sent.append(4)\n",
    "            temp_mask.append(1.)\n",
    "        elif (w == unknown_token):\n",
    "            temp_sent.append(5)\n",
    "            temp_mask.append(0.)\n",
    "        else:\n",
    "            temp_sent.append(5)\n",
    "            temp_mask.append(0.)\n",
    "            \n",
    "    X_test.append(temp_index)\n",
    "    X_test_sent_for_word.append(temp_sent)\n",
    "    X_test_mask.append(temp_mask)\n",
    "    \n",
    "#X_test = np.asarray(X_test)\n",
    "#X_test_sent_for_word = np.asarray(X_test_sent_for_word)\n",
    "#X_test_mask = np.asarray(X_test_mask)\n",
    "\n",
    "y_test = [[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences]\n",
    "\n",
    "# Print an training data example\n",
    "x_example, y_example = X_test[0], y_test[0]\n",
    "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 39, 15405)\n",
      "(39, ?, 512)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "seq_max_len = 39\n",
    "num_sentiment_label = 3\n",
    "num_sentiment_for_word = 6\n",
    "embedding_size = 256\n",
    "num_linear_inside = 256\n",
    "num_ltsm_inside = 256\n",
    "layers = 2\n",
    "alpha  = 0.01\n",
    "dimension = vocabulary_size + num_sentiment_for_word\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    tf_X_train = tf.placeholder(tf.int32, shape=[None, seq_max_len])\n",
    "    tf_X_train_sent_for_word = tf.placeholder(tf.int32, shape=[None, seq_max_len])\n",
    "    tf_X_train_mask = tf.placeholder(tf.float32, shape=[None, seq_max_len])\n",
    "    tf_X_train_labels = tf.placeholder(tf.int32, shape=[None])\n",
    "    tf_y_train = tf.placeholder(tf.int32, shape=[None, seq_max_len])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    embeddings_w = tf.Variable(tf.random_uniform([dimension, embedding_size], -1.0, 1.0))\n",
    "    embeddings_b = tf.Variable(tf.zeros([embedding_size]))\n",
    "    \n",
    "    lm_w = tf.Variable(tf.truncated_normal([2 * num_ltsm_inside, vocabulary_size], \n",
    "                                           stddev=1.0 / math.sqrt(num_ltsm_inside)))\n",
    "    lm_b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    sent_w = tf.Variable(tf.truncated_normal([2 * num_ltsm_inside, num_sentiment_label],\n",
    "                                             stddev=1.0 / math.sqrt(num_ltsm_inside)))\n",
    "    sent_b = tf.Variable(tf.zeros([num_sentiment_label]))\n",
    "    \n",
    "    X_data = tf.one_hot(tf_X_train, vocabulary_size,\n",
    "                        on_value = 1.0,\n",
    "                        off_value = 0.0,\n",
    "                        axis = -1)\n",
    "    X_sent_for_word = tf.one_hot(tf_X_train_sent_for_word, num_sentiment_for_word,\n",
    "                                 on_value = 1.0,\n",
    "                                 off_value = 0.0,\n",
    "                                 axis = -1)\n",
    "    X_labels = tf.one_hot(tf_X_train_labels, num_sentiment_label,\n",
    "                          on_value = 1.0,\n",
    "                          off_value = 0.0,\n",
    "                          axis = -1)\n",
    "    y_labels = tf.one_hot(tf_y_train, vocabulary_size,\n",
    "                          on_value = 1.0,\n",
    "                          off_value = 0.0,\n",
    "                          axis = -1)\n",
    "    X_data = tf.concat(2, [X_data, X_sent_for_word])\n",
    "    \n",
    "\n",
    "    X_data = tf.transpose(X_data, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    X_data = tf.reshape(X_data, [-1, dimension])\n",
    "    X_data = tf.add(tf.matmul(X_data, embeddings_w), embeddings_b)\n",
    "    X_data = tf.nn.relu(X_data)\n",
    "    X_data = tf.split(0, seq_max_len, X_data)\n",
    "    \n",
    "\n",
    "    # Creating the forward and backwards cells\n",
    "    lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_ltsm_inside, forget_bias=1.0)\n",
    "    lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_ltsm_inside, forget_bias=1.0)\n",
    "    # Pass lstm_fw_cell / lstm_bw_cell directly to tf.nn.bidrectional_rnn\n",
    "    # if only a single layer is needed\n",
    "    lstm_fw_multicell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell]*layers)\n",
    "    lstm_bw_multicell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell]*layers)\n",
    "    # Get lstm cell output\n",
    "        \n",
    "    outputs, _, _ = tf.nn.bidirectional_rnn(lstm_fw_multicell,\n",
    "                                            lstm_bw_multicell,\n",
    "                                            X_data,\n",
    "                                            dtype='float32')\n",
    "    # outputs = tf.pack(outputs)\n",
    "    \n",
    "    reconstruction = tf.reshape(outputs, [-1, 2 * num_ltsm_inside])    \n",
    "    reconstruction = tf.add(tf.matmul(reconstruction, lm_w), lm_b)\n",
    "    reconstruction = tf.split(0, seq_max_len, reconstruction)\n",
    "    # change back dimension to [batch_size, n_step, n_input]\n",
    "    reconstruction = tf.pack(reconstruction)\n",
    "    reconstruction = tf.transpose(reconstruction, [1, 0, 2]) \n",
    "    reconstruction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(reconstruction,\n",
    "                                                                            y_labels))\n",
    "    \n",
    "    \n",
    "    # sentiment = tf.transpose(outputs, [1, 0, 2])\n",
    "    # mask = tf.expand_dims(tf_X_train_mask, 2)\n",
    "    # sentiment = tf.multiply(sentiment, mask)\n",
    "    # sentiment = tf.reduce_mean(sentiment, reduction_indices=1)\n",
    "    sentiment = outputs[-1]\n",
    "    sentiment = tf.nn.dropout(sentiment, keep_prob)\n",
    "    sentiment = tf.add(tf.matmul(sentiment, sent_w), sent_b)\n",
    "    prediction = tf.argmax(tf.nn.softmax(sentiment), 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(X_labels, 1)), tf.float32))\n",
    "    correct_prediction = tf.reduce_sum(tf.cast(tf.equal(prediction, tf.argmax(X_labels, 1)), tf.float32))\n",
    "    sentiment = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(sentiment, X_labels))\n",
    "    \n",
    "    cost = alpha * reconstruction + (1 - alpha) * sentiment\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(0.01, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost,\n",
    "                                                                          global_step=global_step)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Cost at step 0 is 1.1704, cost lm is 9.7716, cost sent is 1.0835, accuracy is 0.3300\n",
      "Correct prediction: 0 - 44.0/ batch_size = 100\n",
      "Correct prediction: 100 - 48.0/ batch_size = 100\n",
      "Correct prediction: 200 - 35.0/ batch_size = 100\n",
      "Correct prediction: 300 - 43.0/ batch_size = 100\n",
      "Correct prediction: 400 - 54.0/ batch_size = 100\n",
      "Correct prediction: 500 - 38.0/ batch_size = 100\n",
      "Correct prediction: 600 - 38.0/ batch_size = 100\n",
      "Correct prediction: 700 - 39.0/ batch_size = 100\n",
      "Correct prediction: 800 - 40.0/ batch_size = 100\n",
      "Correct prediction: 900 - 31.0/ batch_size = 100\n",
      "Correct prediction: 1000 - 50.0/ batch_size = 100\n",
      "Correct prediction: 1100 - 51.0/ batch_size = 100\n",
      "Correct prediction: 1200 - 54.0/ batch_size = 100\n",
      "Correct prediction: 1300 - 44.0/ batch_size = 100\n",
      "Correct prediction: 1400 - 52.0/ batch_size = 100\n",
      "Correct prediction: 1500 - 59.0/ batch_size = 100\n",
      "Correct prediction: 1600 - 43.0/ batch_size = 100\n",
      "Correct prediction: 1700 - 58.0/ batch_size = 100\n",
      "Correct prediction: 1800 - 50.0/ batch_size = 100\n",
      "Correct prediction: 1900 - 55.0/ batch_size = 100\n",
      "Correct prediction: 2000 - 54.0/ batch_size = 100\n",
      "Correct prediction: 2100 - 50.0/ batch_size = 100\n",
      "Correct prediction: 2200 - 47.0/ batch_size = 100\n",
      "Correct prediction: 2200 - 76.0/ batch_size = 145\n",
      "1153.0\n",
      "Test accuracy: 0.491684434968\n",
      "Cost at step 1 is 1.1479, cost lm is 9.7690, cost sent is 1.0609, accuracy is 0.5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-37f58915764f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                       \u001b[0mtf_X_train_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                       \u001b[0mtf_y_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                       keep_prob : 0.5})\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cost at step %d is %.4f, cost lm is %.4f, cost sent is %.4f, accuracy is %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    # saver.restore(sess, \"se-v002.ckpt\")\n",
    "    for i in range(10000):\n",
    "        s_start = (i * batch_size) % len(X_train)\n",
    "        if s_start + batch_size >= len(X_train):\n",
    "            s_start = len(X_train) - batch_size\n",
    "        \n",
    "        s_end = s_start + batch_size\n",
    "        \n",
    "        _, c, r, s, a = session.run([optimizer, cost, reconstruction, sentiment, accuracy],\n",
    "                                    feed_dict = {tf_X_train: X_train[s_start:s_end], \n",
    "                                     tf_X_train_sent_for_word: X_train_sent_for_word[s_start:s_end],\n",
    "                                     tf_X_train_mask: X_train_mask[s_start:s_end],\n",
    "                                     tf_X_train_labels: X_train_labels[s_start:s_end],\n",
    "                                     tf_y_train: y_train[s_start:s_end],\n",
    "                                     keep_prob: 0.5})\n",
    "        \n",
    "        print(\"Cost at step %d is %.4f, cost lm is %.4f, cost sent is %.4f, accuracy is %.4f\" %(i, c, r, s, a))\n",
    "        # print(\"Cost at step %d is %.4f, accuracy is %.4f\" %(step, c, a))\n",
    "        if (i % 5 == 0):\n",
    "            step = 0\n",
    "            true_prediction = 0\n",
    "            while step + batch_size <= len(X_test):\n",
    "                correct_pred = session.run(correct_prediction,\n",
    "                                           feed_dict = {tf_X_train: X_test[step:step+batch_size], \n",
    "                                            tf_X_train_sent_for_word: X_test_sent_for_word[step:step+batch_size],\n",
    "                                            tf_X_train_mask: X_test_mask[step:step+batch_size],\n",
    "                                            tf_X_train_labels: X_test_labels[step:step+batch_size],\n",
    "                                            tf_y_train: y_test[step:step+batch_size],\n",
    "                                            keep_prob: 1.0})\n",
    "                print (\"Correct prediction: {0} - {1}/ batch_size = {2}\".format(step,\n",
    "                                                                                correct_pred,\n",
    "                                                                                batch_size))\n",
    "                true_prediction += correct_pred\n",
    "                step += batch_size\n",
    "            \n",
    "            step = step - batch_size\n",
    "            correct_pred = session.run(correct_prediction,\n",
    "                                       feed_dict = {tf_X_train: X_test[step:len(X_test)], \n",
    "                                        tf_X_train_sent_for_word: X_test_sent_for_word[step:len(X_test)],\n",
    "                                        tf_X_train_mask: X_test_mask[step:len(X_test)],\n",
    "                                        tf_X_train_labels: X_test_labels[step:len(X_test)],\n",
    "                                        tf_y_train: y_test[step:len(X_test)],\n",
    "                                        keep_prob: 1.0})\n",
    "            print (\"Correct prediction: {0} - {1}/ batch_size = {2}\".format(step,\n",
    "                                                                            correct_pred,\n",
    "                                                                            len(X_test) - step))\n",
    "            \n",
    "            true_prediction += correct_pred\n",
    "            \n",
    "            print(true_prediction)\n",
    "            print (\"Test accuracy: {}\".format(true_prediction/len(X_test)))\n",
    "            \n",
    "            saver.save(session, \"se-v002.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
