{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from itertools import izip\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from itertools import izip\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "import csv\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import collections\n",
    "import re\n",
    "import preprocess_data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21748\n"
     ]
    }
   ],
   "source": [
    "with open('../encoder/dictionary-id', 'rb') as load_dict:\n",
    "    words_dict = pickle.load(load_dict)\n",
    "\n",
    "print(len(words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "embedding_dict = defaultdict(list)\n",
    "embedding = open('../embedding-results/sswe-u.txt')\n",
    "\n",
    "for line in embedding:\n",
    "    elements = line.split()\n",
    "    for i in range(1, len(elements)):\n",
    "        embedding_dict[elements[0]].append(float(elements[i]))\n",
    "embedding.close()\n",
    "\n",
    "embedding_encoder = defaultdict(list)\n",
    "for k, v in words_dict.iteritems():\n",
    "    if k in embedding_dict:\n",
    "        embedding_encoder[v] = embedding_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\"vocabulary_size\", 21748, \"vocabulary size\")\n",
    "flags.DEFINE_integer(\"n_sentiment_class\", 5, \"number sentiment class\")\n",
    "flags.DEFINE_float(\"starter_learning_rate\", 0.001, \"starter learning rate\")\n",
    "flags.DEFINE_integer(\"training_iters\", 10000, \"number of training iterations\")\n",
    "flags.DEFINE_integer(\"batch_size\", 50, \"size of batch\")\n",
    "flags.DEFINE_integer(\"display_step\", 1, \"number of steps ==> display\")\n",
    "flags.DEFINE_integer(\"seq_max_len\", 30, \"sequence max length\")\n",
    "flags.DEFINE_integer(\"n_hidden\", 200, \"number of hidden neurons\")\n",
    "flags.DEFINE_integer(\"n_hidden_inside\", 100, \"number of hidden neurons before LSTM\")\n",
    "flags.DEFINE_integer(\"n_classes\", 3, \"number of sentiment classes - [0,1,2,3]\")\n",
    "flags.DEFINE_float(\"dropout\", 0.5, \"dropout - probability to keep units\")\n",
    "flags.DEFINE_integer(\"n_add\", 4, \"number of additional features\")\n",
    "flags.DEFINE_float(\"amplified_coefficient\", 10., \"coefficient for adjusting impact of additional features\")\n",
    "flags.DEFINE_boolean(\"train_mode\", True, \"training mode is set to True by default\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"./\", \"directory to checkpoint\")\n",
    "flags.DEFINE_float(\"alpha\", 0.001, \"regularized term for balancing context and sentiment\")\n",
    "flags.DEFINE_boolean(\"using_pos\", False, \"whether using POSTag feature or not\")\n",
    "flags.DEFINE_boolean(\"using_add_features\", False, \"whether using added feature or not\")\n",
    "flags.DEFINE_boolean(\"using_sentiment_masking\", True, \"whether using sentiment masking\")\n",
    "\n",
    "dimension = FLAGS.vocabulary_size + FLAGS.n_sentiment_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequenceData(object):\n",
    "    def __init__(self, max_seq_len=200, min_seq_len=1,\n",
    "                 data_in=None, label_in=None, table=None,\n",
    "                 add_features=None, pos_flag=None):\n",
    "\n",
    "        self.data = []\n",
    "        self.data_2 = []\n",
    "        self.labels = []\n",
    "        self.sequence_length = []\n",
    "        self.add = []\n",
    "\n",
    "        self.one_hot_out = []\n",
    "        self.data_target = []\n",
    "        self.sentiment_masking = []\n",
    "        self.one_hot_target = []\n",
    "\n",
    "        normal_class = \"5\"\n",
    "        depth1 = len(table)\n",
    "        depth2 = FLAGS.n_sentiment_class\n",
    "\n",
    "        if data_in and label_in:\n",
    "\n",
    "            if pos_flag:\n",
    "                raise Exception(\"Please reset pos_flag\")\n",
    "\n",
    "            n_class = FLAGS.n_classes\n",
    "            with open(data_in) as in_data, open(label_in) as in_label:\n",
    "                for x, y in izip(in_data, in_label):\n",
    "                    x = x.strip().split()\n",
    "                    y = y.strip()\n",
    "                    length = len(x)\n",
    "                    assert length >= min_seq_len\n",
    "\n",
    "                    zero_int = 0\n",
    "                    one_int = 1\n",
    "                    zero_mask = 0.01\n",
    "                    one_mask = 0.99\n",
    "\n",
    "                    if length > max_seq_len:\n",
    "                        x = x[:max_seq_len]\n",
    "                        s = []\n",
    "                        s_data_2 = []\n",
    "                        if pos_flag:\n",
    "                            raise Exception(\"Please reset pos_flag\")\n",
    "                        else:\n",
    "                            for i in x:\n",
    "                                s_ = int(i.split(\"/\")[0])\n",
    "                                s__ = table[int(i.split(\"/\")[0])]\n",
    "                                s += [s_]\n",
    "                                s_data_2 += [s__]\n",
    "\n",
    "                        self.sequence_length.append(max_seq_len)\n",
    "                        s_target = []\n",
    "                        if pos_flag:\n",
    "                            raise Exception(\"Please reset pos_flag\")\n",
    "\n",
    "                        else:\n",
    "                            for i in x[1:]:\n",
    "                                s_ = int(i.split(\"/\")[0])\n",
    "                                s_target += [s_]\n",
    "\n",
    "                        s_one_hot = []\n",
    "                        s_one_hot += [zero_int for _ in range(max_seq_len - 1)]\n",
    "                        s_one_hot += [one_int]\n",
    "\n",
    "                        # using sentiment masking:\n",
    "                        if FLAGS.using_sentiment_masking:\n",
    "                            s_sentiment_mask = []\n",
    "                            for i in x:\n",
    "                                if i.split(\"/\")[1] == normal_class:\n",
    "                                    s_sentiment_mask += [zero_mask]\n",
    "                                else:\n",
    "                                    s_sentiment_mask += [one_mask]\n",
    "\n",
    "                        assert len(s_one_hot) == max_seq_len\n",
    "\n",
    "                    elif length == max_seq_len:\n",
    "                        s = []\n",
    "                        s_data_2 = []\n",
    "                        if pos_flag:\n",
    "                            raise Exception(\"Please reset pos_flag\")\n",
    "                        else:\n",
    "                            for i in x:\n",
    "                                s_ = int(i.split(\"/\")[0])\n",
    "                                s__ = table[int(i.split(\"/\")[0])]\n",
    "                                s += [s_]\n",
    "                                s_data_2 += [s__]\n",
    "\n",
    "                        s += [depth1 for _ in range(max_seq_len - length)]\n",
    "                        s_data_2 += [depth2 for _ in range(max_seq_len - length)]\n",
    "                        self.sequence_length.append(length)\n",
    "\n",
    "                        s_target = []\n",
    "                        if pos_flag:\n",
    "                            raise Exception(\"Please reset pos_flag\")\n",
    "\n",
    "                        else:\n",
    "                            for i in x[1:]:\n",
    "                                s_ = int(i.split(\"/\")[0])\n",
    "                                s_target += [s_]\n",
    "\n",
    "                        s_target += [depth1 for _ in range(max_seq_len - length)]\n",
    "                        s_one_hot = []\n",
    "                        s_one_hot += [zero_int for _ in range(length - 1)]\n",
    "                        s_one_hot += [one_int]\n",
    "\n",
    "                        # using sentiment masking:\n",
    "                        if FLAGS.using_sentiment_masking:\n",
    "                            s_sentiment_mask = []\n",
    "                            for i in x:\n",
    "                                if i.split(\"/\")[1] == normal_class:\n",
    "                                    s_sentiment_mask += [zero_mask]\n",
    "                                else:\n",
    "                                    s_sentiment_mask += [one_mask]\n",
    "                            s_sentiment_mask += [zero_mask for _ in range(max_seq_len - length)]\n",
    "                    else:\n",
    "                        s = []\n",
    "                        s_data_2 = []\n",
    "                        if pos_flag:\n",
    "                            raise Exception(\"Please reset pos_flag\")\n",
    "                        else:\n",
    "                            for i in x:\n",
    "                                s_ = int(i.split(\"/\")[0])\n",
    "                                s__ = table[int(i.split(\"/\")[0])]\n",
    "                                s += [s_]\n",
    "                                s_data_2 += [s__]\n",
    "\n",
    "                        s += [depth1 for _ in range(max_seq_len - length)]\n",
    "                        s_data_2 += [depth2 for _ in range(max_seq_len - length)]\n",
    "                        # s += [zero for _ in range(max_seq_len - length)]\n",
    "                        self.sequence_length.append(length)\n",
    "\n",
    "                        s_target = []\n",
    "                        if pos_flag:\n",
    "                            raise Exception(\"Please reset pos_flag\")\n",
    "                        else:\n",
    "                            for i in x[1:]:\n",
    "                                s_ = int(i.split(\"/\")[0])\n",
    "                                s_target += [s_]\n",
    "\n",
    "                        s_target += [depth1 for _ in range(max_seq_len - length)]\n",
    "\n",
    "                        s_one_hot = []\n",
    "                        s_one_hot += [zero_int for _ in range(length - 1)]\n",
    "                        s_one_hot += [one_int]\n",
    "                        s_one_hot += [zero_int for _ in range(max_seq_len - length)]\n",
    "\n",
    "                        # using sentiment masking:\n",
    "                        if FLAGS.using_sentiment_masking:\n",
    "                            s_sentiment_mask = []\n",
    "                            for i in x:\n",
    "                                if i.split(\"/\")[1] == normal_class:\n",
    "                                    s_sentiment_mask += [zero_mask]\n",
    "                                else:\n",
    "                                    s_sentiment_mask += [one_mask]\n",
    "                            s_sentiment_mask += [zero_mask for _ in range(max_seq_len - length)]\n",
    "\n",
    "                    s_one_hot_target = []\n",
    "                    s_one_hot_target += [one_int]\n",
    "                    s_one_hot_target += [zero_int for _ in range(max_seq_len-1)]\n",
    "\n",
    "                    assert len(s) == max_seq_len, \\\n",
    "                    \"-- Error: length of data is not equal to max_seq_len --\"\n",
    "                    \n",
    "                    assert len(s_target) == max_seq_len - 1, \\\n",
    "                    \"-- Error: length of data_target is invalid --\"\n",
    "                    \n",
    "                    assert len(s_one_hot) == max_seq_len, \\\n",
    "                    \"-- Error: length of one_hot is invalid --\"\n",
    "                    \n",
    "                    assert len(s_sentiment_mask) == max_seq_len, \\\n",
    "                    \"-- Error: length of sentiment-masking is invalid --\"\n",
    "                    \n",
    "                    self.one_hot_out.append(s_one_hot)\n",
    "                    self.data_target.append(s_target)\n",
    "                    self.data.append(s)\n",
    "                    self.data_2.append(s_data_2)\n",
    "                    self.sentiment_masking.append(s_sentiment_mask)\n",
    "                    self.one_hot_target.append(s_one_hot_target)\n",
    "\n",
    "                    s_label = [0] * n_class\n",
    "                    s_label[int(y) - 1] = 1\n",
    "                    self.labels.append(s_label)\n",
    "            if FLAGS.using_add_features:\n",
    "                # parsing additional features to sequence data\n",
    "                with open(add_features) as features_add:\n",
    "                    for line in features_add:\n",
    "                        line = line.strip().split()\n",
    "                        add_feat = [FLAGS.amplified_coefficient * float(i) for i in line]\n",
    "                        self.add.append(add_feat)\n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        \"\"\" Return a batch of data. When dataset end is reached, start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                  len(self.data))])\n",
    "        batch_data_2 = (self.data_2[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                      len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                      len(self.data))])\n",
    "        batch_sequence_length = (self.sequence_length[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                                        len(self.data))])\n",
    "\n",
    "        batch_one_hot = (self.one_hot_out[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                            len(self.data))])\n",
    "\n",
    "        batch_target = (self.data_target[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                           len(self.data))])\n",
    "\n",
    "        batch_one_hot_target = (self.one_hot_target[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                                      len(self.data))])\n",
    "\n",
    "        if FLAGS.using_add_features:\n",
    "            batch_add = (self.add[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                    len(self.data))])\n",
    "            self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "            return batch_data, batch_data_2, batch_labels, batch_sequence_length, batch_add, batch_one_hot, batch_target, batch_one_hot_target\n",
    "\n",
    "        if FLAGS.using_sentiment_masking:\n",
    "            batch_sentiment_mask = (self.sentiment_masking[self.batch_id:min(self.batch_id + batch_size,\n",
    "                                                                             len(self.data))])\n",
    "            self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "            return batch_data, batch_data_2, batch_labels, batch_sequence_length, batch_one_hot, batch_target, batch_sentiment_mask, batch_one_hot_target\n",
    "\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_data_2, batch_labels, batch_sequence_length, batch_one_hot, batch_target, batch_one_hot_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse_encoding(x_main, indices1, indices2):\n",
    "    depth1 = FLAGS.vocabulary_size\n",
    "    depth2 = FLAGS.n_sentiment_class\n",
    "    one_hot_main = tf.cast(indices1, tf.float32)\n",
    "    one_hot = tf.one_hot(indices=x_main, depth=depth1, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    two_hot = tf.one_hot(indices=indices2, depth=depth2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    merge_main = tf.concat(2, [one_hot_main, two_hot])\n",
    "    merge = tf.concat(2, [one_hot, two_hot])\n",
    "   \n",
    "    # TODO here\n",
    "    return merge_main, merge\n",
    "\n",
    "\n",
    "def masking(indices, depth):\n",
    "    temp = tf.transpose(indices)\n",
    "    return tf.transpose([temp, ] * depth)\n",
    "\n",
    "\n",
    "def dynamic_rnn(x_main, x, xx, seqlen,\n",
    "                weights, biases, dropout, addition,\n",
    "                one_hot, one_hot_target, sentiment_mask, switch):\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unroll data\n",
    "    \n",
    "    x, x_tmp = sparse_encoding(x_main, x, xx)\n",
    "    x = tf.cast(x, tf.float32)\n",
    "\n",
    "    # one-hot target\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    \n",
    "    x_target = tf.dynamic_partition(x_tmp, one_hot_target, 2)[0]\n",
    "    \n",
    "    x_target = tf.reshape(x_target, [batch_size, -1])\n",
    "\n",
    "    one_hot = masking(one_hot, depth=dimension)\n",
    "    sentiment_mask = masking(sentiment_mask, depth=FLAGS.n_hidden)\n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, 55])\n",
    "    # passing data through hidden layer with RELU activation\n",
    "    x = tf.add(tf.matmul(x, weights['inside']), biases['inside'])\n",
    "    # x = tf.sigmoid(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, FLAGS.seq_max_len, x)\n",
    "    # Define a lstm cell with tensorflow\n",
    "    # lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(FLAGS.n_hidden)\n",
    "\n",
    "    # lstm_cell = tf.nn.rnn_cell.GRUCell(FLAGS.n_hidden)\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(FLAGS.n_hidden)\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.pack(outputs)\n",
    "    # compute the reconstruction\n",
    "    # issues: output is slided by 1 element\n",
    "    # need to share the common LSTM parameters, then using switch may not be appropriate\n",
    "    # thus, we need to consider using sharing parameters features from Tensorflow\n",
    "    if switch:\n",
    "        batch_size = tf.shape(outputs)[1]\n",
    "        # reshaping to (n_step*batch_size, n_hidden)\n",
    "        reconstruction = tf.reshape(outputs, [-1, FLAGS.n_hidden])\n",
    "        reconstruction = tf.add(tf.matmul(reconstruction,\n",
    "                                          weights['reconstruct']),\n",
    "                                biases['reconstruct'])\n",
    "        reconstruction = tf.nn.relu(reconstruction)\n",
    "        # reconstruction = tf.sigmoid(reconstruction)\n",
    "        reconstruction = tf.split(0, FLAGS.seq_max_len, reconstruction)\n",
    "        # change back dimension to [batch_size, n_step, n_input]\n",
    "        reconstruction = tf.pack(reconstruction)\n",
    "        reconstruction = tf.transpose(reconstruction, [1, 0, 2])\n",
    "        # one_hot = tf.to_int32(one_hot)\n",
    "        reconstruction = tf.dynamic_partition(reconstruction, one_hot, 2)[0]\n",
    "        # reconstruction = tf.reshape(reconstruction, [-1, FLAGS.seq_max_len-1, dimension])\n",
    "        # indices = seq_len-1\n",
    "        reconstruction = tf.reshape(reconstruction, [batch_size, -1])\n",
    "        # partition_table = tf.one_hot_table(indices, )\n",
    "        # batch_size = tf.shape(reconstruction)[0]\n",
    "\n",
    "        reconstruction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(reconstruction,\n",
    "                                                                                x_target))\n",
    "        \n",
    "        return reconstruction\n",
    "    else:\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        \n",
    "\n",
    "        outputs *= sentiment_mask\n",
    "        # used = tf.sign(tf.reduce_max(sentiment_mask, reduction_indices=2))\n",
    "        # length = tf.reduce_sum(used, reduction_indices=1, keep_dims=True)\n",
    "        # length = tf.cast(length, tf.float32)\n",
    "        outputs = tf.reduce_sum(outputs, reduction_indices=1)\n",
    "        # outputs /= length\n",
    "        if FLAGS.using_add_features:\n",
    "            outputs = tf.concat(1, [outputs, addition])\n",
    "\n",
    "        outputs = tf.nn.dropout(outputs, dropout)\n",
    "        return tf.matmul(outputs, weights['out']) + biases['out']\n",
    "        '''\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "        outputs *= sentiment_mask\n",
    "\n",
    "        used = tf.sign(tf.reduce_max(sentiment_mask, reduction_indices=2))\n",
    "\n",
    "        length = tf.reduce_sum(used, reduction_indices=1, keep_dims=True)\n",
    "\n",
    "        # length = tf.cast(length, tf.float32)\n",
    "\n",
    "        outputs = tf.reduce_sum(outputs, reduction_indices=1)\n",
    "\n",
    "        outputs /= length\n",
    "\n",
    "        if FLAGS.using_add_features:\n",
    "            outputs = tf.concat(1, [outputs, addition])\n",
    "\n",
    "        outputs = tf.nn.dropout(outputs, dropout)\n",
    "        return tf.matmul(outputs, weights['out']) + biases['out'], length\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_table(source):\n",
    "    dictionary = {}\n",
    "    count = 1\n",
    "    with open(source, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                dictionary[count] = int(line.split()[1])\n",
    "                count += 1\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def table_loading():\n",
    "    source_path = \"../encoder/\"\n",
    "\n",
    "    Table = one_hot_table(source_path + \"dictionary-list\")\n",
    "\n",
    "    print(\"===================\")\n",
    "    print(\"Dynamic RNN for SA\")\n",
    "    print(\"===================\")\n",
    "    print(\"Loading lookup table\")\n",
    "    # print(Table)\n",
    "\n",
    "    # tf Graph input\n",
    "    print(\"check input dimension\")\n",
    "\n",
    "    input_size = len(Table) + FLAGS.n_sentiment_class\n",
    "    print(input_size)\n",
    "    # assert input_size == dimension, \"--check vocabulary length--\"\n",
    "\n",
    "    if FLAGS.using_pos:\n",
    "        raise Exception(\"Please reset pos_flag\")\n",
    "    return input_size, Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_training(dimension, lookup_table):\n",
    "    # clear the default graph stack and reset the global default graph\n",
    "    print(\"Reset the global default graph.. \")\n",
    "    tf.reset_default_graph()\n",
    "    # Passing data\n",
    "    print(\"Passing data\")\n",
    "    source_path = \"../encoder/\"\n",
    "\n",
    "    # load training data without additional features\n",
    "    trainset = SequenceData(max_seq_len=FLAGS.seq_max_len,\n",
    "                            data_in=source_path + \"train-data\",\n",
    "                            label_in=source_path + \"train-label\",\n",
    "                            table=lookup_table,\n",
    "                            add_features=None,\n",
    "                            pos_flag=FLAGS.using_pos)\n",
    "\n",
    "    # load test data without additional features\n",
    "    testset = SequenceData(max_seq_len=FLAGS.seq_max_len,\n",
    "                           data_in=source_path + \"test-data\",\n",
    "                           label_in=source_path + \"test-label\",\n",
    "                           table=lookup_table,\n",
    "                           add_features=None,\n",
    "                           pos_flag=FLAGS.using_pos)\n",
    "\n",
    "    print(\"Check the data size\")\n",
    "    print(\"Training set:\")\n",
    "    print(len(trainset.data), len(trainset.data[0]))\n",
    "\n",
    "    print(\"Test set:\")\n",
    "    print(len(testset.data), len(testset.data[0]))\n",
    "\n",
    "    test_length = len(testset.data)\n",
    "    print(\"Start training ..\")\n",
    "\n",
    "    # Placeholder for data\n",
    "    x_main = tf.placeholder(tf.int32, [None, FLAGS.seq_max_len])\n",
    "    x = tf.placeholder(tf.int32, [None, FLAGS.seq_max_len, 50])\n",
    "    xx = tf.placeholder(tf.int32, [None, FLAGS.seq_max_len])\n",
    "\n",
    "    # Placeholder for label\n",
    "    y = tf.placeholder(\"float\", [None, FLAGS.n_classes])\n",
    "\n",
    "    # A placeholder for indicating each sequence length\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    # Placeholder for one-hot partition matrix\n",
    "    one_hot = tf.placeholder(tf.int32, [None, FLAGS.seq_max_len])\n",
    "\n",
    "    # Placeholder for one-hot target partition matrix\n",
    "    one_hot_target = tf.placeholder(tf.int32, [None, FLAGS.seq_max_len])\n",
    "\n",
    "    # Place holder for sentiment masking\n",
    "    sentiment_mask = tf.placeholder(tf.float32, [None, FLAGS.seq_max_len])\n",
    "\n",
    "    # Placeholder for data target\n",
    "    sent_target = tf.placeholder(\"float\", [None, FLAGS.seq_max_len - 1])\n",
    "    sentence_target = masking(sent_target, depth=dimension)\n",
    "\n",
    "    # Placeholder for dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Placeholder for added data\n",
    "    # addition = tf.placeholder(\"float\", [None, FLAGS.n_add])\n",
    "\n",
    "    # Define weights\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        weights = {\n",
    "            'inside': tf.Variable(tf.random_normal([55, FLAGS.n_hidden_inside])),\n",
    "\n",
    "            'reconstruct': tf.Variable(tf.random_normal([FLAGS.n_hidden, dimension])),\n",
    "\n",
    "            'out': tf.Variable(tf.random_normal([FLAGS.n_hidden, FLAGS.n_classes]))\n",
    "        }\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        biases = {\n",
    "            'inside': tf.Variable(tf.random_normal([FLAGS.n_hidden_inside])),\n",
    "\n",
    "            'reconstruct': tf.Variable(tf.random_normal([dimension])),\n",
    "\n",
    "            'out': tf.Variable(tf.random_normal([FLAGS.n_classes]))\n",
    "        }\n",
    "\n",
    "    # Variables scope for sharing parameters\n",
    "    with tf.variable_scope(\"dynamic_rnn\") as scope:\n",
    "        sentiment = dynamic_rnn(x_main, x, xx, seq_len, weights,\n",
    "                                biases, keep_prob, addition=None, one_hot=one_hot, \n",
    "                                one_hot_target=one_hot_target, sentiment_mask=sentiment_mask,\n",
    "                                switch=False)\n",
    "        \n",
    "        \n",
    "        scope.reuse_variables()\n",
    "        reconstruction = dynamic_rnn(x_main, x, xx, seq_len, weights,\n",
    "                                     biases, keep_prob, addition=None, one_hot=one_hot,\n",
    "                                     one_hot_target=one_hot_target, sentiment_mask=sentiment_mask, \n",
    "                                     switch=True)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope(\"cost_sentiment\"):\n",
    "        cost_sent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(sentiment, y))\n",
    "\n",
    "    with tf.name_scope(\"cost_reconstruction\"):\n",
    "        cost_recon = reconstruction\n",
    "\n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "        cost = FLAGS.alpha * cost_recon + (1 - FLAGS.alpha) * cost_sent\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"learning_rate\"):\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(0.001, global_step, 100, 0.65, staircase=True)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost,\n",
    "                                                                          global_step=global_step)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "\n",
    "        with tf.name_scope(\"correct_prediction\"):\n",
    "            correct_pred = tf.equal(tf.argmax(sentiment, 1), tf.argmax(y, 1))\n",
    "            correct_prediction = tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "   \n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Using defaults to saving all variables\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, \"se-v001-test.ckpt\")\n",
    "        if FLAGS.train_mode:\n",
    "            step = 1\n",
    "            # Keep training until reach max iterations\n",
    "            while step * FLAGS.batch_size < 10000:\n",
    "                if FLAGS.using_add_features:\n",
    "                    batch_x, batch_xx, batch_y, batch_sequence_length, \\\n",
    "                    batch_add, batch_one_hot, batch_target,\\\n",
    "                    batch_sentiment_mask, batch_one_hot_target = trainset.next(FLAGS.batch_size)\n",
    "\n",
    "                else:\n",
    "                    batch_x, batch_xx, batch_y, batch_sequence_length, \\\n",
    "                    batch_one_hot, batch_target, \\\n",
    "                    batch_sentiment_mask, batch_one_hot_target = trainset.next(FLAGS.batch_size)\n",
    "                \n",
    "\n",
    "                tmp = np.zeros(shape=(len(batch_x), 30, 50))\n",
    "                for i in range(len(batch_x)):\n",
    "                    for j in range(len(batch_x[i])):\n",
    "                        tmp[i][j] = embedding_encoder[float(batch_x[i][j])]\n",
    "                # Run optimization (back-prop)\n",
    "                sess.run(optimizer,\n",
    "                         feed_dict={x_main: batch_x,\n",
    "                                    x: tmp,\n",
    "                                    xx: batch_xx,\n",
    "                                    y: batch_y,\n",
    "                                    seq_len: batch_sequence_length,\n",
    "                                    keep_prob: FLAGS.dropout,\n",
    "                                    one_hot: batch_one_hot,\n",
    "                                    sent_target: batch_target,\n",
    "                                    sentiment_mask: batch_sentiment_mask,\n",
    "                                    one_hot_target: batch_one_hot_target})\n",
    "                if step % FLAGS.display_step == 0:\n",
    "                    # Calculate batch accuracy\n",
    "                    acc = sess.run(accuracy,\n",
    "                                   feed_dict={x_main: batch_x,\n",
    "                                              x: tmp, xx: batch_xx,\n",
    "                                              y: batch_y,\n",
    "                                              seq_len: batch_sequence_length,\n",
    "                                              keep_prob: FLAGS.dropout,\n",
    "                                              one_hot: batch_one_hot,\n",
    "                                              sent_target: batch_target,\n",
    "                                              sentiment_mask: batch_sentiment_mask,\n",
    "                                              one_hot_target: batch_one_hot_target})\n",
    "\n",
    "\n",
    "                    # Calculate batch loss\n",
    "                    loss, loss_sent, loss_recon = sess.run([cost, cost_sent, cost_recon],\n",
    "                                                           feed_dict={x_main: batch_x,\n",
    "                                                                  x: tmp,\n",
    "                                                                  xx: batch_xx,\n",
    "                                                                  y: batch_y,\n",
    "                                                                  seq_len: batch_sequence_length,\n",
    "                                                                  keep_prob: FLAGS.dropout,\n",
    "                                                                  one_hot: batch_one_hot,\n",
    "                                                                  sent_target: batch_target,\n",
    "                                                                  sentiment_mask: batch_sentiment_mask,\n",
    "                                                                  one_hot_target: batch_one_hot_target})\n",
    "\n",
    "                    print(\"Iter \" + str(step * FLAGS.batch_size) + \", Loss= \" +\n",
    "                          \"{:.4f}\".format(loss) + \", Loss Sent= \" +\n",
    "                          \"{:.4f}\".format(loss_sent) + \", Loss Recon= \" +\n",
    "                          \"{:.4f}\".format(loss_recon) + \", Accuracy= \" +\n",
    "                          \"{:.4f}\".format(acc) + \", Learning_rate= \" +\n",
    "                          \"{:.6f}\".format(learning_rate.eval()))\n",
    "                    \n",
    "                        \n",
    "                step += 1\n",
    "            print(\"Optimization Finished!\")\n",
    "            saver.save(sess, \"se-v001-test.ckpt\")\n",
    "            # calculate accuracy\n",
    "            step = 1\n",
    "            count = 0\n",
    "            while step * FLAGS.batch_size <= test_length:\n",
    "                test_x, test_xx, test_y, test_sequence_length, \\\n",
    "                test_one_hot, test_target, test_sentiment_mask, \\\n",
    "                test_one_hot_target = testset.next(FLAGS.batch_size)\n",
    "                \n",
    "                tmp = np.zeros(shape=(len(test_x), 30, 50))\n",
    "                for i in range(len(test_x)):\n",
    "                    for j in range(len(test_x[i])):\n",
    "                        tmp[i][j] = embedding_encoder[float(test_x[i][j])]\n",
    "                        \n",
    "                        \n",
    "                correct_pred = sess.run(correct_prediction,\n",
    "                                        feed_dict={x_main: test_x,\n",
    "                                                   x: tmp,\n",
    "                                                   xx: test_xx,\n",
    "                                                   y: test_y, \n",
    "                                                   seq_len: test_sequence_length,\n",
    "                                                   keep_prob: 1.0,\n",
    "                                                   one_hot: test_one_hot,\n",
    "                                                   sent_target: test_target,\n",
    "                                                   sentiment_mask: test_sentiment_mask,\n",
    "                                                   one_hot_target: test_one_hot_target})\n",
    "\n",
    "                print (\"Correct prediction: {0} - {1}/ batch_size = {2}\".format(step,\n",
    "                                                                                correct_pred,\n",
    "                                                                                FLAGS.batch_size))\n",
    "                count += correct_pred\n",
    "                step += 1\n",
    "            print(count)\n",
    "            print (\"Accuracy: {}\".format(count/test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Dynamic RNN for SA\n",
      "===================\n",
      "Loading lookup table\n",
      "check input dimension\n",
      "21753\n",
      "Reset the global default graph.. \n",
      "Passing data\n",
      "Check the data size\n",
      "Training set:\n",
      "6088 30\n",
      "Test set:\n",
      "2336 30\n",
      "Start training ..\n",
      "Iter 50, Loss= 2.5349, Loss Sent= 2.2195, Loss Recon= 317.5924, Accuracy= 0.5400, Learning_rate= 0.000000\n",
      "Iter 100, Loss= 3.1690, Loss Sent= 2.8590, Loss Recon= 312.9485, Accuracy= 0.4200, Learning_rate= 0.000000\n",
      "Iter 150, Loss= 2.8093, Loss Sent= 2.4939, Loss Recon= 317.9645, Accuracy= 0.3600, Learning_rate= 0.000000\n",
      "Iter 200, Loss= 2.5841, Loss Sent= 2.2538, Loss Recon= 332.5337, Accuracy= 0.4800, Learning_rate= 0.000000\n",
      "Iter 250, Loss= 2.5702, Loss Sent= 2.2773, Loss Recon= 295.2059, Accuracy= 0.4600, Learning_rate= 0.000000\n",
      "Iter 300, Loss= 3.2816, Loss Sent= 2.9839, Loss Recon= 300.7059, Accuracy= 0.3600, Learning_rate= 0.000000\n",
      "Iter 350, Loss= 4.0273, Loss Sent= 3.7004, Loss Recon= 330.6517, Accuracy= 0.4400, Learning_rate= 0.000000\n",
      "Iter 400, Loss= 2.8348, Loss Sent= 2.5122, Loss Recon= 325.0400, Accuracy= 0.4600, Learning_rate= 0.000000\n",
      "Iter 450, Loss= 2.6803, Loss Sent= 2.3484, Loss Recon= 334.2435, Accuracy= 0.4400, Learning_rate= 0.000000\n"
     ]
    }
   ],
   "source": [
    "input_size, Table = table_loading()\n",
    "model_training(dimension=input_size, lookup_table=Table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
